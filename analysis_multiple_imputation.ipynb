{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khosh90/prospect/blob/main/analysis_multiple_imputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n",
        "\n",
        "Installations and functions\n"
      ],
      "metadata": {
        "id": "Clevu7AAllA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX_hn1HOlh-p",
        "outputId": "ba8973a5-16a4-4e6a-c44e-01f6653dae3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/Mini_DIVA\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/Mini_DIVA'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels"
      ],
      "metadata": {
        "id": "mx-zz9n1nhcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f396fefb-b054-4165-ca55-58a89c6e7c65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version is: \", tf.__version__)\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from scipy.stats import f\n",
        "from scipy import stats\n",
        "\n"
      ],
      "metadata": {
        "id": "G_ZdbqsOnSD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0294f855-705f-4756-b835-c75b75fe9bcb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version is:  2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_categorical(df, categorical_cols):\n",
        "    df_encoded = df.copy()\n",
        "    encoding_mappings = {}\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        encoder = LabelEncoder()\n",
        "        df_encoded[col] = encoder.fit_transform(df_encoded[col])\n",
        "        encoding_mappings[col] = {\n",
        "            i: category for i, category in enumerate(encoder.classes_)\n",
        "        }\n",
        "\n",
        "    return df_encoded, encoding_mappings\n"
      ],
      "metadata": {
        "id": "mRWx4u_KnVw0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_backto_categorical(df, encoding_mappings):\n",
        "    for col_index, col_name in enumerate(df.columns):\n",
        "        if col_name in encoding_mappings:\n",
        "            mapping = encoding_mappings[col_name]\n",
        "            df[col_name] = df[col_name].apply(lambda x: mapping.get(int(x), x))\n",
        "    return df"
      ],
      "metadata": {
        "id": "G888KUwpK5Vn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def no_impute_variables(df, threshold_identifier=0.9, threshold_cardinality=0.05):\n",
        "    \"\"\"\n",
        "    Identify columns that may not be suitable for imputation.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame representing the dataset.\n",
        "    - threshold_identifier: Threshold for identifying Identifier Columns (default is 0.9).\n",
        "    - threshold_cardinality: Threshold for identifying High Cardinality Categorical Columns (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "    - result: Dictionary containing identified columns in different categories.\n",
        "              Categories include 'Identifier Columns', 'Datetime Columns', 'High Cardinality Columns', and 'Free Text Columns'.\n",
        "    \"\"\"\n",
        "    identifier_columns = []\n",
        "    datetime_columns = []\n",
        "    high_cardinality_columns = []\n",
        "    free_text_columns = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Check for Identifier Columns\n",
        "        uniqueness = df[col].nunique() / len(df[col])\n",
        "        if uniqueness > threshold_identifier:\n",
        "            identifier_columns.append(col)\n",
        "\n",
        "        # Check for Datetime Columns\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "            datetime_columns.append(col)\n",
        "\n",
        "        # Check for High Cardinality Categorical Columns\n",
        "        unique_ratio = df[col].nunique() / len(df[col])\n",
        "        if unique_ratio > threshold_cardinality:\n",
        "            high_cardinality_columns.append(col)\n",
        "\n",
        "        # Check for Free Text Columns\n",
        "        if df[col].dtype == 'O':\n",
        "            free_text_columns.append(col)\n",
        "\n",
        "    return {\n",
        "        \"Identifier Columns\": identifier_columns,\n",
        "        \"Datetime Columns\": datetime_columns,\n",
        "        \"High Cardinality Columns\": high_cardinality_columns,\n",
        "        \"Free Text Columns\": free_text_columns\n",
        "    }\n",
        "\n",
        "# Assuming 'credit' is your DataFrame\n",
        "#result = no_impute_variables(credit)\n",
        "\n",
        "# Display identified columns\n",
        "#for category, columns in result.items():\n",
        "#    print(f\"{category}: {columns}\")\n"
      ],
      "metadata": {
        "id": "TJYhIbZsvAWX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_rubin(selected_dataframes, columns_of_interest, m=10, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform Rubin's Rules for combining results from multiple imputed datasets for numerical variables.\n",
        "\n",
        "    Parameters:\n",
        "    - selected_dataframes: List of DataFrames representing imputed datasets.\n",
        "    - columns_of_interest: List of numerical column names for which Rubin's Rules will be applied.\n",
        "    - m: Number of imputed datasets.\n",
        "    - alpha: Significance level for confidence intervals (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "    - results_summary: DataFrame summarizing the combined results using Rubin's Rules.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate Point Estimates for each imputed dataset and each dataframe\n",
        "    point_estimates_by_dataframe = []\n",
        "    for df in selected_dataframes:\n",
        "      means = df.mean()\n",
        "      point_estimates_by_dataframe.append(means)\n",
        "\n",
        "    # Step 2: Calculate Within-Imputation Variance for each variable in each dataframe\n",
        "    within_imputation_var_by_dataframe = []\n",
        "    for df, means in zip(selected_dataframes, point_estimates_by_dataframe):\n",
        "      within_imputation_var_for_variables = []\n",
        "      for column in columns_of_interest:\n",
        "        squared_deviations = (df[column] - means[column]) ** 2\n",
        "        within_var_for_variable = squared_deviations.mean()\n",
        "        within_imputation_var_for_variables.append(within_var_for_variable)\n",
        "      within_imputation_var_by_dataframe.append(within_imputation_var_for_variables)\n",
        "\n",
        "    # Step 3: Calculate Between-Imputation Variance for each variable in each dataframe\n",
        "    between_imputation_var_for_variables = []\n",
        "    for column in columns_of_interest:\n",
        "      squared_deviations = [(df[column] - means[column]) ** 2 for df, means in zip(selected_dataframes, point_estimates_by_dataframe)]\n",
        "      between_var_for_variable = np.mean(squared_deviations)\n",
        "      between_imputation_var_for_variables.append(between_var_for_variable)\n",
        "\n",
        "    # Step 4: Calculate Pooled Point Estimate\n",
        "    pooled_point_estimate = np.mean(point_estimates_by_dataframe, axis=0)\n",
        "    # Step 5: Calculate Pooled Within-Imputation Variance\n",
        "    pooled_within_var = np.mean(within_imputation_var_by_dataframe, axis=0)\n",
        "    # Step 6: Calculate Pooled Between-Imputation Variance\n",
        "    pooled_between_var = np.mean(between_imputation_var_for_variables)\n",
        "    # Step 7: Calculate Total Variance\n",
        "    total_variance = pooled_within_var + pooled_between_var + (pooled_between_var/m)\n",
        "    # Step 8: Calculate Standard Error\n",
        "    se = np.sqrt(total_variance)\n",
        "    # Step 9: Calculate Degrees of Freedom (if needed)\n",
        "    df_lambda = (pooled_between_var + (pooled_between_var / m)) / total_variance\n",
        "    old_df = (m - 1) / ((df_lambda) ** 2)\n",
        "    # Step 10: Calculate t-statistic and p-value\n",
        "    t_statistic = pooled_point_estimate / se\n",
        "    p_value = 2 * (1 - stats.t.cdf(np.abs(t_statistic), df=old_df))\n",
        "\n",
        "    # Step 11: Calculate 95% confidence interval for the original method\n",
        "    t_critical = stats.t.ppf(1 - alpha / 2, df=old_df)\n",
        "    margin_of_error = t_critical * se\n",
        "    confidence_interval_lower = np.round(pooled_point_estimate - margin_of_error, 2)\n",
        "    confidence_interval_upper = np.round(pooled_point_estimate + margin_of_error, 2)\n",
        "\n",
        "    #confidence_interval = (confidence_interval_lower, confidence_interval_upper)\n",
        "\n",
        "\n",
        "    # Step 12: Summarize Results\n",
        "    results_summary = pd.DataFrame({\n",
        "        'Variable': columns_of_interest,\n",
        "        'Pooled Estimate': pooled_point_estimate,\n",
        "        'SE': se,\n",
        "        't-statistic': t_statistic,\n",
        "        'old df': old_df,\n",
        "        'p-value': p_value,\n",
        "        'CI': list(zip(confidence_interval_lower, confidence_interval_upper))\n",
        "\n",
        "        #'Confidence Interval Lower': confidence_interval_lower,\n",
        "        #'Confidence Interval Upper': confidence_interval_upper\n",
        "        #'CI': confidence_interval\n",
        "    })\n",
        "    return results_summary"
      ],
      "metadata": {
        "id": "sGciWrPiEkqX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_pooling(imputations, categorical_vars, m ):\n",
        "    \"\"\"\n",
        "    Perform Rubin's Rules for combining results from multiple imputed datasets for categorical variables.\n",
        "\n",
        "    Parameters:\n",
        "    - imputation: List of DataFrames representing imputed datasets.\n",
        "    - flat_cat_cols_list: List of categorical column names for which Rubin's Rules will be applied.\n",
        "    - m: Number of imputed datasets.\n",
        "    - decode_func: Function to decode categorical values to original labels (optional).\n",
        "    - theta_0: Null hypothesis is that all parameters are zero\n",
        "\n",
        "    Returns:\n",
        "    - df_results: DataFrame summarizing the combined results using Rubin's Rules for categorical variables.\n",
        "    - wald_results: Dictionary containing the Multivariate Wald Test results.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate Within-Imputation Variance for each categorical variable\n",
        "    within_imputation_var_by_variable = []\n",
        "    for variable in categorical_vars:\n",
        "        within_imputation_var_for_variable = [\n",
        "            (imputations[i][variable].sum() / len(imputations[i][variable])) * (1 - (imputations[i][variable].sum() / len(imputations[i][variable])))\n",
        "            for i in range(m)\n",
        "        ]\n",
        "        within_imputation_var_by_variable.append(within_imputation_var_for_variable)\n",
        "\n",
        "    # Step 2: Calculate Between-Imputation Variance for each categorical variable\n",
        "    between_imputation_var_by_variable = [\n",
        "        np.var([imputations[i][variable].mean() for i in range(m)])\n",
        "        for variable in categorical_vars\n",
        "    ]\n",
        "\n",
        "    # Step 3: Calculate Pooled Within-Imputation Variance for each categorical variable\n",
        "    pooled_within_var_by_variable = np.mean(within_imputation_var_by_variable, axis=1)\n",
        "\n",
        "    # Step 4: Calculate Pooled Between-Imputation Variance for all categorical variables\n",
        "    pooled_between_var = np.mean(between_imputation_var_by_variable)\n",
        "\n",
        "    # Calculate r_1\n",
        "    V_B = pooled_between_var\n",
        "    V_W = pooled_within_var_by_variable\n",
        "    k = len(V_W)\n",
        "\n",
        "    # Multiply the scalar to each element of V_W\n",
        "    scaled_V_W = (1 + 1/m) * V_B * V_W\n",
        "\n",
        "    # Calculate r_1 using the sum of scaled_V_W\n",
        "    r_1 = np.sum(scaled_V_W) / k\n",
        "\n",
        "    # Calculate total variation (V_T)\n",
        "    V_T = (1 + r_1) * pooled_within_var_by_variable\n",
        "\n",
        "    # Assume null hypothesis (all parameters are zero)\n",
        "    theta_0 = np.zeros(k)\n",
        "\n",
        "    # Calculate multivariate Wald statistic (D_1)\n",
        "    theta_bar = np.mean(pooled_within_var_by_variable)\n",
        "    V_T_matrix = np.diag(V_T)\n",
        "    D_1 = (1/k) * np.dot((theta_bar - theta_0).T, np.linalg.inv(V_T_matrix))\n",
        "    D_1 = np.dot(D_1, (theta_bar - theta_0))\n",
        "\n",
        "    # Degrees of freedom\n",
        "    t = k * (m - 1)\n",
        "    t_inv = 1/t\n",
        "    k_inv = 1/k\n",
        "    r_1_inv = 1/r_1\n",
        "    if t >= 4:\n",
        "      v_1 = 4 + (t - 4) * (1 + (1 - 2 * t_inv) * r_1_inv)**2\n",
        "    else:\n",
        "      v_1 = t * (1 + k_inv) * (1 + r_1_inv)**2 / 2\n",
        "    # Calculate p-value\n",
        "    p_value = 1 - f.cdf(D_1, k, v_1)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Multivariate Wald Test Results:\")\n",
        "    #print(f\"Pooled Within-Imputation Variance: {pooled_within_var_by_variable}\")\n",
        "    #print(f\"Pooled Between-Imputation Variance: {pooled_between_var}\")\n",
        "    #print(f\"Relative Increase in Variance (r_1): {r_1}\")\n",
        "    #print(f\"Pooled Total Variance (V_T): {V_T}\")\n",
        "    print(f\"Multivariate Wald Statistic (D_1): {D_1}\")\n",
        "    print(f\"Degrees of Freedom (v_1): {v_1}\")\n",
        "    print(f\"P-Value: {p_value}\")\n",
        "\n",
        "    return {'D_1': D_1, 'v_1': v_1, 'p_value': p_value}\n",
        "\n",
        "# Example usage\n",
        "# Assuming 'imputations' is a list of dataframes and 'categorical_vars' is a list of categorical variable names\n",
        "#calculate_categorical_variances(imputations, categorical_vars, m=3)"
      ],
      "metadata": {
        "id": "9SJTBs4J9dlO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_imputed_data(actual_data, imputed_data, confidence_interval=1.96):\n",
        "    \"\"\"\n",
        "    Evaluate imputed data and calculate various metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_data: DataFrame representing the original dataset with true labels.\n",
        "    - imputed_data: DataFrame representing the imputed dataset.\n",
        "    - confidence_interval: Z-score for calculating lower and upper bounds (default is 1.96 for a 95% confidence interval).\n",
        "\n",
        "    Returns:\n",
        "    - evaluation_results: Dictionary containing evaluation metrics.\n",
        "    \"\"\"\n",
        "    # Placeholder for results\n",
        "    evaluation_results = {}\n",
        "\n",
        "    # Assuming you have your original data stored in actual_data with the true labels\n",
        "    true_labels = actual_data.to_numpy()\n",
        "\n",
        "    # Assuming you have your imputed dataset stored in imputed_data\n",
        "    imputed_data_np = imputed_data.to_numpy()\n",
        "\n",
        "    # Calculate standard error\n",
        "    std_error = np.std(imputed_data_np, axis=0) / np.sqrt(imputed_data_np.shape[0])\n",
        "\n",
        "    # Calculate evaluation metrics with corrected lower and upper bounds\n",
        "    raw_bias = np.mean(imputed_data_np - true_labels)\n",
        "    lower_bound = imputed_data_np - confidence_interval * std_error\n",
        "    upper_bound = imputed_data_np + confidence_interval * std_error\n",
        "    coverage_rate = np.mean((true_labels >= lower_bound) & (true_labels <= upper_bound))\n",
        "    average_width = np.mean(upper_bound - lower_bound)\n",
        "    rmse = np.sqrt(np.mean((imputed_data_np - true_labels)**2))\n",
        "    mse = np.mean((imputed_data_np - true_labels)**2)\n",
        "    r_squared = 1 - mse / np.var(true_labels)\n",
        "    predictions = np.round(imputed_data_np)\n",
        "    accuracy = np.mean(predictions == true_labels)\n",
        "\n",
        "    # Store the results\n",
        "    evaluation_results[\"Results\"] = {\n",
        "        'Raw Bias': raw_bias,\n",
        "        'Coverage Rate': coverage_rate,\n",
        "        'Average Width': average_width,\n",
        "        'Root Mean Squared Error': rmse,\n",
        "        'Accuracy': accuracy,\n",
        "        'Mean Squared Error': mse,\n",
        "        'R-squared': r_squared,\n",
        "    }\n",
        "\n",
        "    return evaluation_results"
      ],
      "metadata": {
        "id": "NTrsZXy_tRa0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n"
      ],
      "metadata": {
        "id": "VGCyPR5lMS0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, X, y, Xinds, yinds = read_dataset(dataset=\"credit\")"
      ],
      "metadata": {
        "id": "ipzDuXrFiafZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_num, credit_cat = get_num_cat_vars(X)\n",
        "credit_encoded, credit_mappings = encode_categorical(X, credit_cat)\n"
      ],
      "metadata": {
        "id": "OWC90qddijQS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can generate imputed dataset with different models. Here I used RandomForest:"
      ],
      "metadata": {
        "id": "JWMf9SkdifIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming credit_encoded is your original dataset\n",
        "fraction = 0.25\n",
        "data, data_ind = set_fraction_missing(credit_encoded, fraction=fraction, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest imputer\n",
        "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(), random_state=42)\n",
        "\n",
        "# Initialize an empty list to store the imputed datasets\n",
        "imputed_datasets = []\n",
        "\n",
        "# Generate three different imputed datasets using a for loop\n",
        "for _ in range(3):\n",
        "    imputed_data = pd.DataFrame(rf_imputer.fit_transform(data), columns=credit_encoded.columns)\n",
        "    imputed_datasets.append(imputed_data)"
      ],
      "metadata": {
        "id": "BnWD8COn5TOy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_datasets"
      ],
      "metadata": {
        "id": "LiObgEqV6x45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa4fa57-5b7d-417a-f342-ee1fd4463fc9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[       A1       A2        A3   A4   A5     A6    A7       A8    A9  A10  A11  \\\n",
              " 0    1.00  30.8300   0.00000  1.0  0.0  12.00  7.00  2.51915  0.64  1.0  1.0   \n",
              " 1    0.49  58.6700   4.46000  1.0  0.0  10.00  3.00  3.04000  1.00  1.0  6.0   \n",
              " 2    0.00  24.5000   0.50000  1.0  0.0   4.55  3.00  1.50000  1.00  0.0  0.0   \n",
              " 3    1.00  27.8300   1.54000  1.0  0.0  12.00  7.00  3.75000  1.00  1.0  5.0   \n",
              " 4    0.86  20.1700   5.15535  1.0  0.0   6.10  7.00  1.71000  0.27  0.0  0.0   \n",
              " ..    ...      ...       ...  ...  ...    ...   ...      ...   ...  ...  ...   \n",
              " 648  1.00  26.1187  10.08500  2.0  2.0   4.00  3.00  1.25000  0.22  0.0  0.0   \n",
              " 649  0.00  22.6700   3.58405  1.0  0.0   5.44  5.69  2.00000  0.70  1.0  2.0   \n",
              " 650  0.67  25.2500   4.37105  2.0  2.0   5.00  2.00  2.00000  0.00  1.0  1.0   \n",
              " 651  1.00  28.2645   0.20500  1.0  0.0   0.00  5.05  0.04000  0.00  0.0  0.0   \n",
              " 652  1.00  35.7235   3.37500  1.0  0.0   1.00  2.55  8.29000  0.00  0.0  0.0   \n",
              " \n",
              "      A12   A13     A14      A15  \n",
              " 0    0.0  0.00  202.00     0.00  \n",
              " 1    0.0  0.00   43.00  1046.95  \n",
              " 2    0.0  0.00  280.00   824.00  \n",
              " 3    1.0  0.00  344.29   363.87  \n",
              " 4    0.0  2.00  120.00    24.82  \n",
              " ..   ...   ...     ...      ...  \n",
              " 648  0.0  0.05  260.00   127.46  \n",
              " 649  1.0  0.00  200.00   447.93  \n",
              " 650  1.0  0.00  200.00     1.00  \n",
              " 651  0.0  0.00  280.00   750.00  \n",
              " 652  1.0  0.00  350.53     0.00  \n",
              " \n",
              " [653 rows x 15 columns],\n",
              "        A1       A2        A3   A4   A5     A6    A7      A8    A9  A10  A11  \\\n",
              " 0    1.00  30.8300   0.00000  1.0  0.0  12.00  7.00  0.7162  0.41  1.0  1.0   \n",
              " 1    0.51  58.6700   4.46000  1.0  0.0  10.00  3.00  3.0400  1.00  1.0  6.0   \n",
              " 2    0.00  24.5000   0.50000  1.0  0.0   4.57  3.00  1.5000  1.00  0.0  0.0   \n",
              " 3    1.00  27.8300   1.54000  1.0  0.0  12.00  7.00  3.7500  1.00  1.0  5.0   \n",
              " 4    0.86  20.1700   4.58505  1.0  0.0   6.21  7.00  1.7100  0.26  0.0  0.0   \n",
              " ..    ...      ...       ...  ...  ...    ...   ...     ...   ...  ...  ...   \n",
              " 648  1.00  24.8767  10.08500  2.0  2.0   4.00  3.00  1.2500  0.10  0.0  0.0   \n",
              " 649  0.00  22.6700   5.57685  1.0  0.0   6.32  5.46  2.0000  0.33  1.0  2.0   \n",
              " 650  0.71  25.2500   4.11905  2.0  2.0   5.00  2.00  2.0000  0.00  1.0  1.0   \n",
              " 651  1.00  29.3994   0.20500  1.0  0.0   0.00  5.43  0.0400  0.00  0.0  0.0   \n",
              " 652  1.00  35.3818   3.37500  1.0  0.0   1.00  2.89  8.2900  0.00  0.0  0.0   \n",
              " \n",
              "      A12  A13     A14      A15  \n",
              " 0    0.0  0.0  202.00     0.00  \n",
              " 1    0.0  0.0   43.00  1311.01  \n",
              " 2    0.0  0.0  280.00   824.00  \n",
              " 3    1.0  0.0  360.72   302.43  \n",
              " 4    0.0  2.0  120.00    64.25  \n",
              " ..   ...  ...     ...      ...  \n",
              " 648  0.0  0.0  260.00    32.24  \n",
              " 649  1.0  0.0  200.00   169.11  \n",
              " 650  1.0  0.0  200.00     1.00  \n",
              " 651  0.0  0.0  280.00   750.00  \n",
              " 652  1.0  0.0  359.16     0.00  \n",
              " \n",
              " [653 rows x 15 columns],\n",
              "        A1       A2        A3   A4   A5     A6    A7      A8    A9  A10  A11  \\\n",
              " 0    1.00  30.8300   0.00000  1.0  0.0  12.00  7.00  1.5163  0.56  1.0  1.0   \n",
              " 1    0.50  58.6700   4.46000  1.0  0.0  10.00  3.00  3.0400  1.00  1.0  6.0   \n",
              " 2    0.00  24.5000   0.50000  1.0  0.0   3.67  3.00  1.5000  1.00  0.0  0.0   \n",
              " 3    1.00  27.8300   1.54000  1.0  0.0  12.00  7.00  3.7500  1.00  1.0  5.0   \n",
              " 4    0.77  20.1700   4.70395  1.0  0.0   5.78  7.00  1.7100  0.31  0.0  0.0   \n",
              " ..    ...      ...       ...  ...  ...    ...   ...     ...   ...  ...  ...   \n",
              " 648  1.00  24.9639  10.08500  2.0  2.0   4.00  3.00  1.2500  0.22  0.0  0.0   \n",
              " 649  0.00  22.6700   4.28065  1.0  0.0   5.93  5.36  2.0000  0.75  1.0  2.0   \n",
              " 650  0.78  25.2500   4.51420  2.0  2.0   5.00  2.00  2.0000  0.00  1.0  1.0   \n",
              " 651  1.00  27.4561   0.20500  1.0  0.0   0.00  5.66  0.0400  0.00  0.0  0.0   \n",
              " 652  1.00  35.4478   3.37500  1.0  0.0   1.00  2.32  8.2900  0.00  0.0  0.0   \n",
              " \n",
              "      A12   A13     A14      A15  \n",
              " 0    0.0  0.00  202.00     0.00  \n",
              " 1    0.0  0.00   43.00  1205.30  \n",
              " 2    0.0  0.00  280.00   824.00  \n",
              " 3    1.0  0.00  365.96   404.51  \n",
              " 4    0.0  2.00  120.00     1.35  \n",
              " ..   ...   ...     ...      ...  \n",
              " 648  0.0  0.03  260.00    72.83  \n",
              " 649  1.0  0.00  200.00  1714.90  \n",
              " 650  1.0  0.00  200.00     1.00  \n",
              " 651  0.0  0.00  280.00   750.00  \n",
              " 652  1.0  0.00  342.31     0.00  \n",
              " \n",
              " [653 rows x 15 columns]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation function:"
      ],
      "metadata": {
        "id": "opgA9wTViCd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_of_interest = credit_num\n",
        "# Placeholder for results\n",
        "all_evaluation_results = {}\n",
        "confidence_interval = 1.96\n",
        "# Loop over each imputed dataset\n",
        "for i, imputed_data in enumerate(imputed_datasets):\n",
        "    # Extract the columns of interest from the imputed dataset\n",
        "    imputed_data_subset = imputed_data[columns_of_interest]\n",
        "\n",
        "    # Call the evaluate_imputed_data function for each dataset\n",
        "    evaluation_results = evaluate_imputed_data(X[columns_of_interest], imputed_data_subset, confidence_interval)\n",
        "\n",
        "    # Store the results in the dictionary\n",
        "    all_evaluation_results[f'Imputation_{i + 1}'] = evaluation_results\n",
        "\n",
        "# Print or analyze the evaluation results as needed\n",
        "for imputation_name, results in all_evaluation_results.items():\n",
        "    print(f\"\\nResults for {imputation_name}:\")\n",
        "    for key, value in results[\"Results\"].items():\n",
        "        print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "cVa6knMmxJfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55553910-41fe-42d7-afb8-9d995a03c15b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for Imputation_1:\n",
            "Raw Bias: -8.98461336906585\n",
            "Coverage Rate: 0.7883614088820827\n",
            "Average Width: 160.21338720722738\n",
            "Root Mean Squared Error: 1285.3310834365232\n",
            "Accuracy: 0.38315467075038284\n",
            "Mean Squared Error: 1652075.9940481067\n",
            "R-squared: 0.7085307287860814\n",
            "\n",
            "Results for Imputation_2:\n",
            "Raw Bias: -2.5787945022970886\n",
            "Coverage Rate: 0.7895865237366003\n",
            "Average Width: 162.8323583774413\n",
            "Root Mean Squared Error: 1346.6964272889738\n",
            "Accuracy: 0.38192955589586525\n",
            "Mean Squared Error: 1813591.2672728861\n",
            "R-squared: 0.6800352242533929\n",
            "\n",
            "Results for Imputation_3:\n",
            "Raw Bias: -0.5119541500765681\n",
            "Coverage Rate: 0.7895865237366003\n",
            "Average Width: 159.97334552766785\n",
            "Root Mean Squared Error: 1276.317701666271\n",
            "Accuracy: 0.38529862174578866\n",
            "Mean Squared Error: 1628986.8755866725\n",
            "R-squared: 0.7126042511634848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credit_cat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iIdubrhKlvb",
        "outputId": "1e2ed3ac-8fbd-4a2c-8cb2-af76e0d12818"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A11', 'A12', 'A13']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wald Test for cateforical variables:"
      ],
      "metadata": {
        "id": "m-NKc9qTi1DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wald_results = categorical_pooling(imputed_datasets, credit_cat, m=3)\n",
        "\n",
        "print(\"\\nWald Test Results:\")\n",
        "print(wald_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_ye3iywGvIJ",
        "outputId": "6e57047e-057e-4b7c-e580-b56068aadadb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multivariate Wald Test Results:\n",
            "Multivariate Wald Statistic (D_1): 66.19364452783535\n",
            "Degrees of Freedom (v_1): 80509776.43010186\n",
            "P-Value: 1.1102230246251565e-16\n",
            "\n",
            "Wald Test Results:\n",
            "{'D_1': 66.19364452783535, 'v_1': 80509776.43010186, 'p_value': 1.1102230246251565e-16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different models can be generated, and we can assess our models using these functions."
      ],
      "metadata": {
        "id": "qUpXG65Pi5Lb"
      }
    }
  ]
}